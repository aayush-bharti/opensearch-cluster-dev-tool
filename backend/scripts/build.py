# Copyright OpenSearch Contributors
# SPDX-License-Identifier: Apache-2.0

import subprocess
import os
import tempfile
import logging
import shutil
from typing import Dict, Any
from botocore.exceptions import ClientError, NoCredentialsError, PartialCredentialsError
from datetime import datetime
from scripts.s3_upload import S3Uploader
from constants import Status, ResultFields, TaskTypes, ConfigFields, ErrorMessages
from scripts.aws_credentials_manager import AWSCredentialsManager

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OpenSearchBuilder:
    """OpenSearch builder that automatically uses AWS credentials from ~/.aws/credentials"""
    
    def __init__(self, manifest_yml: str, workflow_timestamp: str = None, config: Dict[str, Any] = None):
        self.manifest_yml = manifest_yml
        self.workflow_timestamp = workflow_timestamp
        self.config = config or {}
        self.temp_dir = tempfile.mkdtemp()
        self.manifest_path = os.path.join(self.temp_dir, "manifest.yml")
        
        # Use shared AWS credentials manager to validate credentials
        self.aws = AWSCredentialsManager()
        self.aws.validate_aws_credentials()
        
        # Initialize AWS session/region from manager
        self.session = self.aws.get_session()
        self.region = self.aws.get_region()
        
    
    # run the build process
    def run_build(self) -> Dict[str, Any]:
        """Run the OpenSearch build process"""
        try:
            # Capture start time
            task_started_at = datetime.now().strftime("%m/%d/%y, %H:%M:%S")
            logger.info(f"ğŸ“… Task started at: {task_started_at}")
            
            # Clone opensearch-build repo if not present
            if not os.path.exists("opensearch-build"):
                logger.info("ğŸ“¦ Cloning opensearch-build repository (build)...")
                subprocess.run([
                    "git", "clone", "https://github.com/opensearch-project/opensearch-build.git"
                ], check=True)
                logger.info("âœ… Repository cloned for Docker build")
            else:
                logger.info("ğŸ“¦ Pulling latest changes from opensearch-build repository (build)...")
                subprocess.run([
                    "git", "pull", "origin", "main"
                ], check=True, cwd="opensearch-build")
                logger.info("âœ… Latest changes pulled for build")

            # Pull the correct Docker image for OpenSearch build
            docker_image = "opensearchstaging/ci-runner:ci-runner-al2-opensearch-build-v1"
            logger.info(f"ğŸ³ Pulling Docker image: {docker_image}")
            subprocess.run([
                "docker", "pull", docker_image
            ], check=True)
            logger.info(f"âœ… Pulled Docker image: {docker_image}")

            # Run the build inside a Docker container
            docker_cmd = [
                "docker", "run", "--rm",
                "-v", f"{os.path.abspath('opensearch-build')}:/opensearch-build",
                "-v", f"{os.path.abspath(self.manifest_path)}:/opensearch-build/manifest.yml",
                "-w", "/opensearch-build",
                "-e", "JAVA_HOME=/opt/java/openjdk-24",
                docker_image,
                "./build.sh", "/opensearch-build/manifest.yml",
                "-p", "linux",
                "-a", "arm64",
                "--continue-on-error",
                "-d", "tar",
                "-s"
            ]
            
            # Add custom build parameters if provided
            custom_params = self.config.get(ConfigFields.CUSTOM_BUILD_PARAMS, [])
            if custom_params:
                logger.info(f"ğŸ”§ Adding custom build parameters: {custom_params}")
                for param in custom_params:
                    if param.get("value") and param["value"].strip():
                        param_value = param["value"].strip()
                        docker_cmd.append(param_value)
                        logger.info(f"ğŸ“ Added parameter: {param_value}")
            
            logger.info(f"ğŸ³ Running build: {' '.join(docker_cmd)}")
            result = subprocess.run(
                docker_cmd,
                capture_output=True,
                text=True
            )
            logger.info(f"ğŸ³ Build stdout:\n{result.stdout}")
            if result.stderr:
                logger.warning(f"ğŸ³ Build stderr:\n{result.stderr}")
            if result.returncode != 0:
                logger.error(f"âŒ Build failed with code {result.returncode}")
                raise Exception(f"Build failed: {result.stderr}")

            # Extract build manifest path from build output
            build_manifest_path = None
            for line in result.stdout.split('\n'):
                if 'Created build manifest' in line:
                    build_manifest_path = line.split('Created build manifest')[1].strip()
                    logger.info(f"ğŸ“ Found build manifest: {build_manifest_path}")
                    break
            
            if not build_manifest_path:
                logger.error("âŒ Could not find build manifest path in build output")
                logger.error("   Expected to find a line containing 'Created build manifest'")
                logger.error("   Build output:")
                logger.error(result.stdout)
                raise Exception("Could not find build manifest path in build output")

            # Run assemble.sh to create the tarball in a named container
            container_name = f"opensearch-assemble-{int(datetime.now().timestamp())}"
            assemble_cmd = [
                "docker", "run", "--name", container_name,
                "-v", f"{os.path.abspath('opensearch-build')}:/opensearch-build",
                "-w", "/opensearch-build",
                "-e", "JAVA_HOME=/opt/java/openjdk-24",
                docker_image,
                "./assemble.sh", build_manifest_path
            ]
            logger.info(f"ğŸ³ Running assemble: {' '.join(assemble_cmd)}")
            assemble_result = subprocess.run(
                assemble_cmd,
                capture_output=True,
                text=True
            )
            logger.info(f"ğŸ³ assemble stdout:\n{assemble_result.stdout}")
            if assemble_result.stderr:
                logger.warning(f"ğŸ³ assemble stderr:\n{assemble_result.stderr}")
            if assemble_result.returncode != 0:
                logger.error(f"âŒ assemble failed with code {assemble_result.returncode}")
                # Clean up container if it exists
                subprocess.run(["docker", "rm", "-f", container_name], capture_output=True)
                raise Exception(f"assemble failed: {assemble_result.stderr}")

            # Find the resulting tarball by parsing assemble output
            container_tarball_path = None
            for line in assemble_result.stdout.split('\n'):
                if 'Published' in line and '.tar.gz' in line:
                    container_tarball_path = line.split('Published')[1].strip().rstrip('.')
                    logger.info(f"ğŸ“¦ Found build artifact in container: {container_tarball_path}")
                    break

            if not container_tarball_path:
                logger.error("âŒ Could not find build artifact in assemble output")
                logger.error("   Expected to find a line containing 'Published' and '.tar.gz'")
                logger.error("   Assemble output:")
                logger.error(assemble_result.stdout)
                # Clean up container if it exists
                subprocess.run(["docker", "rm", "-f", container_name], capture_output=True)
                raise Exception("Could not find build artifact (.tar.gz) in assemble output")

            # Copy the tarball from Docker container to host
            host_tarball_path = os.path.join(os.getcwd(), os.path.basename(container_tarball_path))
            docker_cp_cmd = ["docker", "cp", f"{container_name}:{container_tarball_path}", host_tarball_path]
            logger.info(f"ğŸ“‹ Copying artifact from container: {' '.join(docker_cp_cmd)}")
            cp_result = subprocess.run(docker_cp_cmd, capture_output=True, text=True)
            if cp_result.returncode != 0:
                logger.error(f"âŒ Failed to copy artifact from container: {cp_result.stderr}")
                # Clean up container if it exists
                subprocess.run(["docker", "rm", "-f", container_name], capture_output=True)
                raise Exception(f"Failed to copy artifact from container: {cp_result.stderr}")
            logger.info(f"âœ… Copied artifact to host: {host_tarball_path}")

            # Clean up the container
            subprocess.run(["docker", "rm", "-f", container_name], capture_output=True)

            # Capture completion time and create timing data
            task_completed_at = datetime.now().strftime("%m/%d/%y, %H:%M:%S")
            logger.info(f"ğŸ“… Task completed at: {task_completed_at}")
            
            # Create timing data for S3 upload
            timing_data = {
                "started_at": task_started_at,
                "completed_at": task_completed_at
            }
            # Store timing data as instance variable for use in upload methods
            self.timing_data = timing_data

            # Upload the local copy to S3
            logger.info("â˜ï¸ Uploading to S3...")
            s3_info = self.upload_tarball_to_s3(host_tarball_path)

            tarball_path = host_tarball_path

            # Upload output to S3
            logger.info("ğŸ“¤ Uploading build command and output to S3...")
            build_command = " ".join(docker_cmd)
            assemble_command = " ".join(assemble_cmd)
            
            # Combine both build.sh and assemble.sh outputs
            build_output = result.stdout
            build_stderr = result.stderr or ""
            assemble_output = assemble_result.stdout
            assemble_stderr = assemble_result.stderr or ""
            
            # Create combined output with clear separation
            combined_output = f"=== BUILD.SH OUTPUT ===\n{build_output}\n"
            if build_stderr:
                combined_output += f"=== BUILD.SH STDERR ===\n{build_stderr}\n"
            combined_output += f"\n=== ASSEMBLE.SH OUTPUT ===\n{assemble_output}\n"
            if assemble_stderr:
                combined_output += f"=== ASSEMBLE.SH STDERR ===\n{assemble_stderr}\n"
            
            # Create combined command
            combined_command = f"# Build command:\n{build_command}\n\n# Assemble command:\n{assemble_command}"
            
            build_config = {ConfigFields.MANIFEST_YML: self.manifest_yml}
            
            bucket_name = self.config.get(ConfigFields.S3_BUCKET, "")
            if not bucket_name or bucket_name.strip() == "":
                logger.warning("âš ï¸ S3 bucket not found in config")
            
            logger.info(f"ğŸ“¤ Using S3 bucket: {bucket_name}")
            uploader = S3Uploader(bucket_name=bucket_name, workflow_timestamp=self.workflow_timestamp)
            output_s3_info = uploader.upload_output_txt(
                TaskTypes.BUILD, 
                combined_output, 
                "",
                combined_command
            )
            
            # Prepare build result
            build_result = {
                ResultFields.STATUS: Status.SUCCESS,
                ResultFields.MESSAGE: "Build completed successfully with S3 upload",
                ResultFields.TARBALL_PATH: tarball_path,
                ResultFields.COMMAND: combined_command,
                ResultFields.S3_INFO: s3_info,
                ResultFields.OUTPUT: combined_output
            }
            # Upload build results metadata to S3
            if s3_info:
                build_result["s3_info"] = s3_info
                timestamp = s3_info.get("timestamp")
                if timestamp:
                    logger.info("ğŸ“ Uploading build results metadata to S3...")
                    # Use timing data from instance variable
                    results_s3_info = self.upload_build_results_to_s3(build_result, timestamp, self.timing_data)
                    if results_s3_info:
                        build_result.update(results_s3_info)
                        logger.info("âœ… Build results metadata uploaded successfully")
                logger.info("âœ… Build completed successfully with S3 upload")
            else:
                build_result[ResultFields.MESSAGE] = "Build completed successfully (S3 upload failed)"
                logger.warning("âš ï¸ Build completed but S3 upload failed")
            
            # Add timing information to build result
            build_result["task_started_at"] = task_started_at
            build_result["task_completed_at"] = task_completed_at
            
            # Add output to S3 info to build result
            if output_s3_info:
                build_result["output_s3_info"] = output_s3_info
                logger.info("âœ… Build command and output uploaded to S3")
            else:
                logger.warning("âš ï¸ Build command and output upload failed")
            
            return build_result
        except Exception as e:
            logger.error(f"âŒ Build failed: {str(e)}")
            raise
    
    # save the manifest to a temporary file
    def save_manifest(self):
        """Save the manifest YAML to a temporary file."""
        with open(self.manifest_path, 'w') as f:
            f.write(self.manifest_yml)
        logger.info(f"ğŸ“ Manifest saved to {self.manifest_path}")
    
    # upload the manifest to S3
    def upload_manifest_to_s3(self):
        """Upload manifest to S3."""
        try:
            logger.info("ğŸ“„ Starting manifest upload to S3...")
            bucket_name = self.config.get(ConfigFields.S3_BUCKET, "")
            uploader = S3Uploader(bucket_name=bucket_name, workflow_timestamp=self.workflow_timestamp)
            result = uploader.upload_build_manifest(self.manifest_path)
            logger.info("âœ… Manifest uploaded to S3 successfully")
            return result
        except Exception as e:
            logger.error(f"âŒ Manifest upload failed: {e}")
            return {"error": str(e), "uploaded": False}
    
    # upload the tarball to S3
    def upload_tarball_to_s3(self, tarball_path: str) -> Dict[str, str]:
        """Upload tarball to S3 and return S3 info."""
        try:
            bucket_name = self.config.get(ConfigFields.S3_BUCKET, "")
            uploader = S3Uploader(bucket_name=bucket_name, workflow_timestamp=self.workflow_timestamp)
            return uploader.upload_build_artifact(tarball_path)
        except Exception as e:
            logger.error(f"âŒ S3 upload failed: {e}")
    
    # upload the build results to S3
    def upload_build_results_to_s3(self, build_result: Dict[str, Any], timestamp: str, timing_data: Dict[str, Any] = None) -> Dict[str, str]:
        """Save build results metadata to S3 in build-results folder."""
        try:
            bucket_name = self.config.get(ConfigFields.S3_BUCKET, "")
            uploader = S3Uploader(bucket_name=bucket_name, workflow_timestamp=self.workflow_timestamp)
            
            # Create build results metadata
            build_metadata = {
                ResultFields.BUILD_STATUS: build_result.get(ResultFields.STATUS),
                ResultFields.BUILD_MESSAGE: build_result.get(ResultFields.MESSAGE),
                ResultFields.BUILD_MANIFEST_PATH: build_result.get(ResultFields.BUILD_MANIFEST_PATH),
                ResultFields.TARBALL_PATH: build_result.get(ResultFields.TARBALL_PATH),
                ResultFields.S3_INFO: build_result.get(ResultFields.S3_INFO, {}),
                ResultFields.CONFIG: self.config
            }
            
            # Use the new upload method with timing data
            result = uploader.upload_task_results(TaskTypes.BUILD, build_metadata, timing_data)
            
            return {
                "results_s3_uri": result["s3_uri"],
                "results_https_url": result["https_url"],
                "results_s3_key": result["s3_key"]
            }
            
        except Exception as e:
            logger.error(f"âŒ Failed to upload build results to S3: {e}")
            return {}
    
    # cleanup the files and repositories
    def cleanup(self):
        """Clean up files and repositories."""
        try:
            shutil.rmtree(self.temp_dir)
            if os.path.exists("opensearch-build"):
                shutil.rmtree("opensearch-build")

            # Clean up tarball files
            for file in os.listdir("."):
                if file.endswith(".tar.gz"):
                    try:
                        os.remove(file)
                        logger.info(f"ğŸ§¹ Removed: {file}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ Failed to remove {file}: {e}")

            logger.info("ğŸ§¹ Cleanup completed")
        except Exception as e:
            logger.warning(f"âš ï¸ Cleanup failed: {str(e)}")

# run the build process
def run_build_process(config: Dict[str, Any], workflow_timestamp: str = None) -> Dict[str, Any]:
    """
    Build OpenSearch with the provided manifest using automatic AWS credentials.
    
    Args:
        config: Build configuration containing manifest_yml and S3 bucket
        workflow_timestamp: Shared timestamp for the workflow
    
    Returns:
        Dictionary containing build results including S3 upload info
    """
    logger.info("ğŸš€ Starting OpenSearch build with auto AWS credentials...")
    
    # Extract manifest_yml from config
    manifest_yml = config.get(ConfigFields.MANIFEST_YML)
    if not manifest_yml:
        raise ValueError("manifest_yml is required in config")
    
    # Capture start time
    task_started_at = datetime.now().isoformat()
    logger.info(f"ğŸ“… Task started at: {task_started_at}")
    
    # initialize the builder
    builder = OpenSearchBuilder(manifest_yml, workflow_timestamp, config)
    
    try:
        # save the manifest to a temporary file
        builder.save_manifest()
        # upload the manifest to S3
        manifest_upload_result = builder.upload_manifest_to_s3()
        # run the build process
        build_result = builder.run_build()
        
        # Capture completion time
        task_completed_at = datetime.now().isoformat()
        logger.info(f"ğŸ“… Task completed at: {task_completed_at}")
        
        # Add timing information to build result
        build_result["task_started_at"] = task_started_at
        build_result["task_completed_at"] = task_completed_at
        
        # Add manifest upload info to build result
        if manifest_upload_result:
            build_result["manifest_s3_info"] = manifest_upload_result
            if not manifest_upload_result.get("uploaded", True):  # Check if upload failed
                logger.warning("âš ï¸ Manifest upload failed but continuing with build")
        
        return build_result
    finally:
        builder.cleanup()